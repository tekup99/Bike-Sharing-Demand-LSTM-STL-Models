{"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"4.0.5"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":36778,"sourceType":"datasetVersion","datasetId":28865}],"dockerImageVersionId":30618,"isInternetEnabled":true,"language":"r","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/tarktunataalt/bike-sharing-demand-lstm-stl-models?scriptVersionId=185958174\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"Previous studies have conducted a comprehensive Exploratory Data Analysis (EDA) and developed various models to forecast bike-sharing demand. These studies included the use of SARIMAX and hybrid models, as well as STL decomposition combined with machine learning models.\n\n1. [ Exploratory Data Analysis (EDA)](https://www.kaggle.com/code/tarktunataalt/washington-dc-capital-bikeshare-eda)\n1. [SARIMAX and Hybrid Models](https://www.kaggle.com/code/tarktunataalt/bike-sharing-demand-sarimax-hybrid-models)\n1. [ STL and Hybrid Models](https://www.kaggle.com/code/tarktunataalt/bike-sharing-demand-stl-hybrid-models)\n\nIn this notebook, the focus is on implementing Long Short-Term Memory (LSTM) networks to improve forecasting accuracy. LSTM models are well-suited for time series data as they can capture long-term dependencies and patterns. Both plain LSTM models and hybrid models, where LSTM is applied to the residuals obtained from STL decomposition, will be explored.\n\nThe performance of these models will be compared with previous approaches to identify the best model for predicting bike-sharing demand.\n\n","metadata":{}},{"cell_type":"code","source":"# Load required libraries\nlibraries <- c(\n  \"psych\",\n  \"dplyr\",\n  \"magrittr\",\n  \"ggplot2\",\n  \"gridExtra\",\n  \"grid\",\n  \"patchwork\",\n  \"lmtest\",\n  \"zoo\",\n  \"xgboost\",\n  \"Metrics\",\n  \"plotly\",\n  \"knitr\",\n  \"forecast\",\n  \"randomForest\",\n  \"gbm\",\n  \"lightgbm\",\n  \"keras\",\n  \"caret\",\n  \"dplyr\",\n  \"nortsTest\",\n  \"tseries\",\n  \"urca\",\n  \"reshape2\",\n  \"catboost\"\n)\n\nload_libraries <- function(libraries) {\n  for (lib in libraries) {\n    if (!require(lib, character.only = TRUE)) {\n      suppressMessages(suppressWarnings(install.packages(lib, dependencies = TRUE)))\n      suppressPackageStartupMessages(library(lib, character.only = TRUE))\n    } else {\n      suppressPackageStartupMessages(library(lib, character.only = TRUE))\n    }\n  }\n}\n\nload_libraries(libraries)\n\n# Load and preprocess data\ndata <- read.csv(\"/kaggle/input/bike-sharing-dataset/day.csv\", header = TRUE)\ndata %<>% select(-c(\"instant\", \"casual\", \"registered\"))\ndata$date <- as.Date(data$dteday, format = \"%Y-%m-%d\")\ndata$dteday <- NULL\n","metadata":{"_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","_execution_state":"idle","execution":{"iopub.status.busy":"2024-06-28T15:19:11.088471Z","iopub.execute_input":"2024-06-28T15:19:11.12364Z","iopub.status.idle":"2024-06-28T15:19:11.177164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create lag 1 feature\ndata$cnt_lag1 <- lag(data$cnt, 1)\ndata$cnt_lag1[is.na(data$cnt_lag1)] <- 0\n\n# Create dummy variables\ndummy_vars <- c(\"season\", \"mnth\", \"weekday\", \"weathersit\")\ndata <- data %>% mutate(across(all_of(dummy_vars), as.factor))\n\n# Label encoding for specific variables\ndata$yr <- as.numeric(data$yr)\ndata$holiday <- as.numeric(data$holiday)\ndata$workingday <- as.numeric(data$workingday)\n\n# Convert dummy variables\ndummies <- dummyVars(~., data = data[, dummy_vars])\ndummy_data <- predict(dummies, newdata = data[, dummy_vars])\ndata <- cbind(data, dummy_data)\ndata <- data[, !names(data) %in% dummy_vars]\n\n# Split data into training and testing sets\ntrain_size <- floor(0.8 * nrow(data))\ntrain_indices <- 1:train_size\ntest_indices <- (train_size + 1):nrow(data)\n\ntrain_data <- data[train_indices, ]\ntest_data <- data[test_indices, ]\n\n# Select features for LSTM\nfeatures <- colnames(data)[!colnames(data) %in% c(\"cnt\", \"date\", \"Time\")]\n\nx_train <- train_data[, features]\ny_train <- train_data$cnt\nx_test <- test_data[, features]\ny_test <- test_data$cnt\n","metadata":{"execution":{"iopub.status.busy":"2024-06-28T15:19:11.180115Z","iopub.execute_input":"2024-06-28T15:19:11.181554Z","iopub.status.idle":"2024-06-28T15:19:11.284587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# STL decomposition\ntrain_ts <- ts(train_data$cnt, start = c(2011, 1), frequency = 30)\ndecomposed <- stl(train_ts, s.window = \"periodic\")\nstl_trend <- decomposed$time.series[, \"trend\"]\nstl_seasonal <- decomposed$time.series[, \"seasonal\"]\nstl_residuals <- decomposed$time.series[, \"remainder\"]\n\n# Forecast the trend and seasonal components for the test period\nn_test <- length(test_data$cnt)\nstl_forecast_trend <- rep(tail(stl_trend, 1), n_test)\nseason_length <- frequency(train_ts)\nstl_seasonal_cycle <- rep(stl_seasonal[(length(stl_seasonal) - season_length + 1):length(stl_seasonal)], length.out = n_test)\nstl_forecast_seasonal <- rep(stl_seasonal_cycle, length.out = n_test)\n\n# Combine forecasted trend and seasonal components\nstl_forecast <- stl_forecast_trend + stl_forecast_seasonal\n\n# Prepare the residuals for LSTM\ny_train_stl <- stl_residuals\ny_test_stl <- test_data$cnt - stl_forecast\n","metadata":{"execution":{"iopub.status.busy":"2024-06-28T15:19:11.28721Z","iopub.execute_input":"2024-06-28T15:19:11.288594Z","iopub.status.idle":"2024-06-28T15:19:11.330451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scale data function\nscale_data <- function(train, test, feature_range = c(0, 1)) {\n  scaler <- list()\n  scaled_train <- train\n  scaled_test <- test\n  for (feature in features) {\n    x <- train[, feature]\n    fr_min <- feature_range[1]\n    fr_max <- feature_range[2]\n    std_train <- (x - min(x)) / (max(x) - min(x))\n    std_test <- (test[, feature] - min(x)) / (max(x) - min(x))\n    scaled_train[, feature] <- std_train * (fr_max - fr_min) + fr_min\n    scaled_test[, feature] <- std_test * (fr_max - fr_min) + fr_min\n    scaler[[feature]] <- list(min = min(x), max = max(x))\n  }\n  return(list(scaled_train = scaled_train,\n              scaled_test = scaled_test,\n              scaler = scaler))\n}\nScaled <- scale_data(x_train, x_test, c(-1, 1))\nx_train <- as.matrix(Scaled$scaled_train)\nx_test <- as.matrix(Scaled$scaled_test)\n\n# Scale y_train and y_test for both plain LSTM and STL + LSTM\ny_scaler <- list(min = min(y_train), max = max(y_train))\ny_train_scaled <- (y_train - y_scaler$min) / (y_scaler$max - y_scaler$min) * 2 - 1\ny_test_scaled <- (y_test - y_scaler$min) / (y_scaler$max - y_scaler$min) * 2 - 1\n\n# Scale y_train_stl and y_test_stl for STL + LSTM\ny_scaler_stl <- list(min = min(y_train_stl), max = max(y_train_stl))\ny_train_stl_scaled <- (y_train_stl - y_scaler_stl$min) / (y_scaler_stl$max - y_scaler_stl$min) * 2 - 1\ny_test_stl_scaled <- (y_test_stl - y_scaler_stl$min) / (y_scaler_stl$max - y_scaler_stl$min) * 2 - 1\n\n# Convert data to 3D tensor format\ndim(x_train) <- c(nrow(x_train), ncol(x_train), 1)\ndim(x_test) <- c(nrow(x_test), ncol(x_test), 1)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-28T15:19:11.3332Z","iopub.execute_input":"2024-06-28T15:19:11.334643Z","iopub.status.idle":"2024-06-28T15:19:11.403886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define and compile the model\nbatch_size <- 1\nunits <- 50\n\nmodel <- keras_model_sequential()\n\nmodel %>%\n  layer_lstm(units, batch_input_shape = c(batch_size, ncol(x_train), 1), stateful = TRUE, return_sequences = TRUE) %>%\n  layer_dropout(rate = 0.2) %>%\n  layer_lstm(units, stateful = TRUE) %>%\n  layer_dropout(rate = 0.2) %>%\n  layer_dense(units = 1)\n\nmodel %>% compile(\n  loss = 'mse',\n  optimizer = optimizer_adam(learning_rate = 0.002),\n  metrics = c(\"mae\")\n)\nsummary(model)\n\n# Train the model for plain LSTM\nEpochs <- 50\nhistory <- model %>% fit(x_train, y_train_scaled, epochs = Epochs, batch_size = batch_size, verbose = 1, shuffle = FALSE)\nmodel %>% reset_states()\n\n# Train the model for STL + LSTM\nhistory_stl <- model %>% fit(x_train, y_train_stl_scaled, epochs = Epochs, batch_size = batch_size, verbose = 1, shuffle = FALSE)\nmodel %>% reset_states()\n","metadata":{"execution":{"iopub.status.busy":"2024-06-28T15:19:11.406518Z","iopub.execute_input":"2024-06-28T15:19:11.407947Z","iopub.status.idle":"2024-06-28T15:34:11.432434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to invert scaling\ninvert_scaling <- function(scaled, scaler, feature_range = c(-1, 1)) {\n  min <- scaler$min\n  max <- scaler$max\n  t <- length(scaled)\n  mins <- feature_range[1]\n  maxs <- feature_range[2]\n  inverted_dfs <- numeric(t)\n  \n  for(i in 1:t) {\n    X <- (scaled[i] - mins) / (maxs - mins)\n    rawValues <- X * (max - min) + min\n    inverted_dfs[i] <- rawValues\n  }\n  return(inverted_dfs)\n}\n\n# Predictions for plain LSTM\npredictions_lstm <- numeric(nrow(x_test))\nfor (i in 1:nrow(x_test)) {\n  X <- array(x_test[i, , ], dim = c(1, ncol(x_test), 1))\n  yhat <- model %>% predict(X, batch_size = batch_size)\n  \n  # Convert the predicted value back to original scale\n  yhat <- invert_scaling(yhat, y_scaler, c(-1, 1))\n  predictions_lstm[i] <- yhat\n}\n\n# Predictions for STL + LSTM\npredictions_stl_lstm <- numeric(nrow(x_test))\nfor (i in 1:nrow(x_test)) {\n  X <- array(x_test[i, , ], dim = c(1, ncol(x_test), 1))\n  yhat <- model %>% predict(X, batch_size = batch_size)\n  \n  # Convert the predicted value back to original scale\n  yhat <- invert_scaling(yhat, y_scaler_stl, c(-1, 1))\n  predictions_stl_lstm[i] <- yhat\n}\n","metadata":{"execution":{"iopub.status.busy":"2024-06-28T15:34:11.436606Z","iopub.execute_input":"2024-06-28T15:34:11.438115Z","iopub.status.idle":"2024-06-28T15:34:33.572831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combine STL forecast and LSTM residuals for STL + LSTM model\nfinal_forecast <- stl_forecast + predictions_stl_lstm\n\n# Visualize the predictions for plain LSTM\ntest_data$LSTM_Predictions <- predictions_lstm\n\n# Visualize the predictions for STL + LSTM\ntest_data$STL_LSTM_Predictions <- final_forecast\n\n# Calculate RMSE using Metrics library for plain LSTM\nlstm_rmse_value <- rmse(y_test, predictions_lstm)\nprint(paste(\"Root Mean Squared Error (RMSE) for LSTM:\", lstm_rmse_value))\n\n# Calculate RMSE using Metrics library for STL + LSTM\nstl_lstm_rmse_value <- rmse(test_data$cnt, final_forecast)\nprint(paste(\"Root Mean Squared Error (RMSE) for STL + LSTM:\", stl_lstm_rmse_value))\n\n# Plot the predictions for plain LSTM\ntrain_df <- data.frame(date = train_data$date, cnt = train_data$cnt, type = \"Train\")\ntest_df <- data.frame(date = test_data$date, cnt = test_data$cnt, type = \"Test\")\nlstm_forecast_df <- data.frame(date = test_data$date, cnt = predictions_lstm, type = \"LSTM Forecast\")\n\nggplot() +\n  geom_line(data = train_df, aes(x = date, y = cnt, color = 'Train'), size = 0.5) +\n  geom_line(data = test_df, aes(x = date, y = cnt, color = 'Actual Test'), size = 0.5) +\n  geom_line(data = lstm_forecast_df, aes(x = date, y = cnt, color = 'LSTM Forecast'), size = 0.5) +\n  labs(title = 'LSTM Model Forecast vs Actual',\n       x = 'Date',\n       y = 'Count') +\n  scale_color_manual(values = c('Train' = 'blue', 'Actual Test' = 'red', 'LSTM Forecast' = 'green'),\n                     breaks = c('Train', 'Actual Test', 'LSTM Forecast')) +  \n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  guides(color = guide_legend(title = NULL))\n\n# Plot the predictions for STL + LSTM\nstl_lstm_forecast_df <- data.frame(date = test_data$date, cnt = final_forecast, type = \"STL + LSTM Forecast\")\n\nggplot() +\n  geom_line(data = train_df, aes(x = date, y = cnt, color = 'Train'), size = 0.5) +\n  geom_line(data = test_df, aes(x = date, y = cnt, color = 'Actual Test'), size = 0.5) +\n  geom_line(data = stl_lstm_forecast_df, aes(x = date, y = cnt, color = 'STL + LSTM Forecast'), size = 0.5) +\n  labs(title = 'STL and LSTM Hybrid Model Forecast vs Actual',\n       x = 'Date',\n       y = 'Count') +\n  scale_color_manual(values = c('Train' = 'blue', 'Actual Test' = 'red', 'STL + LSTM Forecast' = 'green'),\n                     breaks = c('Train', 'Actual Test', 'STL + LSTM Forecast')) +  \n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  guides(color = guide_legend(title = NULL))\n","metadata":{"execution":{"iopub.status.busy":"2024-06-28T15:34:33.576955Z","iopub.execute_input":"2024-06-28T15:34:33.578391Z","iopub.status.idle":"2024-06-28T15:34:34.845834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When comparing the two models, it is observed that the LSTM model (RMSE: 1849.13) has a lower error rate. However, despite the STL + LSTM model (RMSE: 2042.22) having a higher error rate, it is noted for producing more reliable short-term forecasts. Therefore, for short-term predictions, the STL + LSTM model might be preferred. The short-term reliability of the STL + LSTM model can be attributed to better capturing seasonal components and more accurately predicting short-term fluctuations.","metadata":{}}]}